{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class FeatureExtractor:\n",
        "    def __init__(self):\n",
        "        # Use ResNet18 as feature extractor\n",
        "        self.model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
        "        # Remove the last fully connected layer\n",
        "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
        "        self.model.eval()\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224,224)),\n",
        "            # transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                               std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def extract_features(self, image):\n",
        "        with torch.no_grad():\n",
        "            # Convert NumPy array to PIL Image\n",
        "            if isinstance(image, np.ndarray):\n",
        "                image = Image.fromarray(image)\n",
        "\n",
        "            image = self.transform(image).unsqueeze(0)\n",
        "            features = self.model(image)\n",
        "            return features.squeeze().numpy()\n",
        "\n",
        "class LwPClassifier:\n",
        "    def __init__(self, feature_dim=512, num_classes=10, lambda_reg=0.1):\n",
        "        self.weights = np.zeros((num_classes, feature_dim))\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.feature_extractor = FeatureExtractor()\n",
        "\n",
        "    def fit(self, X, y, sample_weights=None):\n",
        "        if sample_weights is None:\n",
        "            sample_weights = np.ones(len(X))\n",
        "\n",
        "        # For each class\n",
        "        for c in range(self.weights.shape[0]):\n",
        "            # Get samples for current class\n",
        "            mask = (y == c)\n",
        "            if not np.any(mask):\n",
        "                continue\n",
        "\n",
        "            X_c = X[mask]\n",
        "            weights_c = sample_weights[mask]\n",
        "\n",
        "            # Weighted average of features for this class\n",
        "            self.weights[c] = np.average(X_c, axis=0, weights=weights_c)\n",
        "\n",
        "        # L2 normalization of weight vectors\n",
        "        norms = np.linalg.norm(self.weights, axis=1, keepdims=True)\n",
        "        self.weights = self.weights / (norms + 1e-8)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Compute similarities with all class prototypes\n",
        "        similarities = np.dot(X, self.weights.T)\n",
        "        return np.argmax(similarities, axis=1)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # Compute similarities and convert to probabilities using softmax\n",
        "        similarities = np.dot(X, self.weights.T)\n",
        "        exp_sim = np.exp(similarities / self.lambda_reg)\n",
        "        return exp_sim / exp_sim.sum(axis=1, keepdims=True)\n",
        "\n",
        "def update_model(current_model, new_data, predicted_labels, confidence_threshold=0.8):\n",
        "    \"\"\"Update model with pseudo-labeled data using confidence thresholding\"\"\"\n",
        "    probs = current_model.predict_proba(new_data)\n",
        "    max_probs = np.max(probs, axis=1)\n",
        "\n",
        "    # Filter samples based on confidence\n",
        "    confident_mask = max_probs >= confidence_threshold\n",
        "    if not np.any(confident_mask):\n",
        "        # If no confident samples, lower the threshold adaptively\n",
        "        confidence_threshold = np.percentile(max_probs, 70)\n",
        "        confident_mask = max_probs >= confidence_threshold\n",
        "\n",
        "    confident_data = new_data[confident_mask]\n",
        "    confident_labels = predicted_labels[confident_mask]\n",
        "    confident_weights = max_probs[confident_mask]\n",
        "\n",
        "    # Update model using weighted samples\n",
        "    current_model.fit(confident_data, confident_labels, sample_weights=confident_weights)\n",
        "    return current_model\n",
        "\n",
        "# Define the feature extraction process\n",
        "def process_dataset(data, feature_extractor):\n",
        "    \"\"\"Process a dataset and extract features\"\"\"\n",
        "    features_list = []\n",
        "    for img in data:\n",
        "        features = feature_extractor.extract_features(img)\n",
        "        features_list.append(features)\n",
        "    return np.array(features_list)\n",
        "\n",
        "# Task 1: Same distribution datasets\n",
        "def task1(base_path, feature_extractor):\n",
        "    models = []\n",
        "    accuracies = np.zeros((10, 10))  # 10 models x 10 datasets\n",
        "\n",
        "    # Initialize with labeled dataset D1\n",
        "\n",
        "    # Load dataset from serialized file\n",
        "    dataset_path = os.path.join(base_path, '1_train_data.tar.pth')\n",
        "    dataset = torch.load(dataset_path)  # Load the serialized dataset\n",
        "    print(dataset.keys())  # Check the available keys: should print 'data' and 'targets'\n",
        "\n",
        "    # Extract data and targets\n",
        "    data, targets = dataset['data'], dataset['targets']  # data: list of images, targets: labels\n",
        "\n",
        "    # Process the features from the loaded data\n",
        "    D1_features = process_dataset(data, feature_extractor)  # Process features directly from the loaded data\n",
        "    D1_labels = targets  # Assign targets as labels (already in memory)\n",
        "\n",
        "    # Initialize first model\n",
        "    model = LwPClassifier()\n",
        "    model.fit(D1_features, D1_labels)\n",
        "    models.append(model)\n",
        "\n",
        "    test_features = []\n",
        "    test_labels = []\n",
        "\n",
        "    for j in range(1, 11):\n",
        "        # Load dataset from serialized file\n",
        "        dataset_path = os.path.join('./drive/MyDrive/mini-project-2/dataset/part_one_dataset/eval_data', f'{j}_eval_data.tar.pth')\n",
        "        dataset = torch.load(dataset_path)  # Load the serialized dataset\n",
        "        print(dataset.keys())  # Check the available keys: should print 'data' and 'targets'\n",
        "\n",
        "        # Extract data and targets\n",
        "        data, targets = dataset['data'], dataset['targets']  # data: list of images, targets: labels\n",
        "\n",
        "        # Process the features from the loaded data\n",
        "        x = process_dataset(data, feature_extractor)  # Process features directly from the loaded data\n",
        "        test_features.append(x)\n",
        "        test_labels.append(targets)  # Assign targets as labels (already in memory)\n",
        "\n",
        "    accuracies[0, 0] = (model.predict(test_features[0]) == test_labels[0]).mean()\n",
        "\n",
        "    # Iteratively process D2 to D10\n",
        "    for i in range(2, 11):\n",
        "        prev_model = models[-1]\n",
        "\n",
        "        # # Process current dataset\n",
        "        # current_features = process_dataset(os.path.join(base_path, f'D{i}'))\n",
        "\n",
        "        # Load dataset from serialized file\n",
        "        dataset_path = os.path.join(base_path, f'{i}_train_data.tar.pth')\n",
        "        dataset = torch.load(dataset_path)  # Load the serialized dataset\n",
        "        print(dataset.keys())  # Check the available keys: should print 'data' and 'targets'\n",
        "\n",
        "        # Extract data and targets\n",
        "        data= dataset['data']  # data: list of images\n",
        "\n",
        "        # Process the features from the loaded data\n",
        "        current_features = process_dataset(data, feature_extractor)  # Process features directly from the loaded data\n",
        "\n",
        "        # Get predictions from previous model\n",
        "        predictions = prev_model.predict(current_features)\n",
        "\n",
        "        # Update model\n",
        "        new_model = update_model(prev_model, current_features, predictions)\n",
        "        models.append(new_model)\n",
        "\n",
        "        # Evaluate on all previous datasets\n",
        "        for j in range(1, i+1):\n",
        "            accuracies[i-1, j-1] = (new_model.predict(test_features[j-1]) == test_labels[j-1]).mean()\n",
        "\n",
        "    return models, accuracies\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Base path to datasets\n",
        "base_path1 = \"./drive/MyDrive/mini-project-2/dataset/part_one_dataset/train_data\"\n",
        "\n",
        "# Create feature extractor\n",
        "feature_extractor = FeatureExtractor()\n",
        "\n",
        "# Task 1: Same distribution datasets\n",
        "print(\"Running Task 1...\")\n",
        "task1_models, task1_accuracies = task1(base_path1, feature_extractor)\n",
        "\n",
        "print(\"\\nTask 1 Accuracy Matrix:\")\n",
        "print(task1_accuracies)\n",
        "\n",
        "# Optional: Save accuracy matrices\n",
        "np.save('task1_accuracies.npy', task1_accuracies)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84ad0aa9-2495-4ddb-f5d6-6ced1f2c22f3",
        "id": "m3joG0rWtSSx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Task 1...\n",
            "dict_keys(['data', 'targets'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-a7ec885bb380>:124: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  dataset = torch.load(dataset_path)  # Load the serialized dataset\n",
            "<ipython-input-7-a7ec885bb380>:145: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  dataset = torch.load(dataset_path)  # Load the serialized dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['data', 'targets'])\n",
            "dict_keys(['data', 'targets'])\n",
            "dict_keys(['data', 'targets'])\n",
            "dict_keys(['data', 'targets'])\n",
            "dict_keys(['data', 'targets'])\n",
            "dict_keys(['data', 'targets'])\n",
            "dict_keys(['data', 'targets'])\n",
            "dict_keys(['data', 'targets'])\n",
            "dict_keys(['data', 'targets'])\n",
            "dict_keys(['data', 'targets'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-a7ec885bb380>:167: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  dataset = torch.load(dataset_path)  # Load the serialized dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['data'])\n",
            "dict_keys(['data'])\n",
            "dict_keys(['data'])\n",
            "dict_keys(['data'])\n",
            "dict_keys(['data'])\n",
            "dict_keys(['data'])\n",
            "dict_keys(['data'])\n",
            "dict_keys(['data'])\n",
            "dict_keys(['data'])\n",
            "\n",
            "Task 1 Accuracy Matrix:\n",
            "[[0.8152 0.     0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
            " [0.7916 0.8    0.     0.     0.     0.     0.     0.     0.     0.    ]\n",
            " [0.7752 0.7916 0.7932 0.     0.     0.     0.     0.     0.     0.    ]\n",
            " [0.774  0.7816 0.7896 0.7828 0.     0.     0.     0.     0.     0.    ]\n",
            " [0.7744 0.7908 0.784  0.788  0.7848 0.     0.     0.     0.     0.    ]\n",
            " [0.7656 0.7808 0.7788 0.78   0.7756 0.778  0.     0.     0.     0.    ]\n",
            " [0.7684 0.7768 0.778  0.778  0.7788 0.776  0.7728 0.     0.     0.    ]\n",
            " [0.7636 0.7764 0.7736 0.774  0.78   0.7748 0.7636 0.7648 0.     0.    ]\n",
            " [0.762  0.772  0.7788 0.7796 0.7796 0.7764 0.77   0.7664 0.7576 0.    ]\n",
            " [0.7572 0.7744 0.776  0.7764 0.7752 0.774  0.7688 0.7644 0.7596 0.7868]]\n"
          ]
        }
      ]
    }
  ]
}